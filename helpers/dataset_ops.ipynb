{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5d7e7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re # Regex Operations\n",
    "from pathlib import Path\n",
    "import helpers.project_config as cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9714c752",
   "metadata": {},
   "source": [
    "## General Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dd1038f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_dataset(path: str | Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    suffix = path.suffix.lower()\n",
    "\n",
    "    if suffix == \".csv\":\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "    raise ValueError(f\"Unsupported input format: {suffix} for {path} \\n Use .csv\")\n",
    "\n",
    "\n",
    "def _write_dataset(df: pd.DataFrame, path: str | Path) -> None:\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    suffix = path.suffix.lower()\n",
    "\n",
    "    if suffix == \".csv\":\n",
    "        df.to_csv(path, index=False)\n",
    "        return\n",
    "\n",
    "    raise ValueError(f\"Unsupported output format: {suffix} for {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f31e1b",
   "metadata": {},
   "source": [
    "## Transformation\n",
    "contains reusable transformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4586b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataframe_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize DataFrame column names by:\n",
    "      - replacing whitespace with underscores\n",
    "      - converting uppercase letters to lowercase\n",
    "\n",
    "    Whitespace runs (e.g., multiple spaces, tabs) are collapsed into a single \"_\".\n",
    "\n",
    "    Args:\n",
    "        df: Input pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        The DataFrame with normalized column names.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If df is not a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(f\"df must be a pandas DataFrame, got {type(df)!r}\")\n",
    "\n",
    "    _WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "    # \\s means “any whitespace character” (space, tab, newline, etc.)\n",
    "    # + means “one or more of the previous character”\n",
    "\n",
    "    def _normalize(name) -> str:\n",
    "        text = str(name).strip()\n",
    "        text = _WHITESPACE_RE.sub(\"_\", text)\n",
    "        return text.lower()\n",
    "\n",
    "    new_columns = [_normalize(col) for col in df.columns]\n",
    "    return df.rename(columns=dict(zip(df.columns, new_columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb97da0",
   "metadata": {},
   "source": [
    "### Smoke Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d7f059e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ normalize_dataframe_columns smoke tests passed\n"
     ]
    }
   ],
   "source": [
    "# Smoke tests for normalize_dataframe_columns() \n",
    "def run_normalize_dataframe_columns_tests() -> None:\n",
    "    # Basic case\n",
    "    df = pd.DataFrame(columns=[\"First Name\", \"Last Name\"])\n",
    "    out = normalize_dataframe_columns(df)\n",
    "    assert list(out.columns) == [\"first_name\", \"last_name\"]\n",
    "\n",
    "    # Edge case 1: multiple whitespace types collapse to single underscore (space/tab/newline)\n",
    "    df = pd.DataFrame(columns=[\"A   B\", \"C\\tD\", \"E\\nF\", \"G \\t \\n H\"])\n",
    "    out = normalize_dataframe_columns(df)\n",
    "    assert list(out.columns) == [\"a_b\", \"c_d\", \"e_f\", \"g_h\"]\n",
    "\n",
    "    # Edge case 2: non-string column names (should not crash; should stringify)\n",
    "    df = pd.DataFrame(columns=[1, (\"A\", \"B\"), 3.14])\n",
    "    out = normalize_dataframe_columns(df)\n",
    "    assert list(out.columns) == [\"1\", \"('a',_'b')\", \"3.14\"]\n",
    "\n",
    "    # Edge case 3: leading/trailing whitespace gets stripped before replacement\n",
    "    df = pd.DataFrame(columns=[\"  Hello World  \", \"\\tFoo Bar\\t\"])\n",
    "    out = normalize_dataframe_columns(df)\n",
    "    assert list(out.columns) == [\"hello_world\", \"foo_bar\"]\n",
    "\n",
    "    # Already-normalized should stay normalized\n",
    "    df = pd.DataFrame(columns=[\"already_ok\", \"ok2\"])\n",
    "    out = normalize_dataframe_columns(df)\n",
    "    assert list(out.columns) == [\"already_ok\", \"ok2\"]\n",
    "\n",
    "    # Type error on wrong input\n",
    "    try:\n",
    "        normalize_dataframe_columns([\"not\", \"a\", \"df\"])  \n",
    "        assert False, \"Expected TypeError for non-DataFrame input\"\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    print(\"✅ normalize_dataframe_columns smoke tests passed\")\n",
    "\n",
    "def basic_sanity_checks() -> None:\n",
    "    # TERM_SIZES attribute\n",
    "    if not hasattr(cfg, \"TERM_SIZES\"):\n",
    "        raise AttributeError(\"cfg.TERM_SIZES is missing in helpers/project_config.py\")\n",
    "\n",
    "    term_sizes = list(cfg.TERM_SIZES)\n",
    "    if not term_sizes:\n",
    "        raise ValueError(\"cfg.TERM_SIZES is empty\")\n",
    "\n",
    "run_normalize_dataframe_columns_tests()\n",
    "basic_sanity_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc3b368",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "orchestration (read -> transform -> write)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ef8756",
   "metadata": {},
   "source": [
    "### Project level dataset fix (adding term, comment density)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2143fa4",
   "metadata": {},
   "source": [
    "**Specifications**\n",
    "\n",
    "\n",
    "Problem: \n",
    "`output/interim_results/aggregated_metrics_v4_061025.csv` has no term column\n",
    "`output/interim_results/cleaned_aggregated_metrics_v4_061025.csv` has not all projects anymore\n",
    "\n",
    "Goal: file with all projects and term column stored under `project_level_dataset_path_raw`\n",
    "\n",
    "Process: add term column (integer 1, 2, 3 or 4) to `/output/interim_results/aggregated_metrics_v4_061025.csv` based on `TERM_SIZES` list from `project_config.py` and store it to new path. Sum of integers in `TERM_SIZES` should equal the number of projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b19e730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_term_column_to_project_dataset() -> None:\n",
    "    \"\"\"\n",
    "    Adds a 'term' column to the project-level dataset based on TERM_SIZES in config.\n",
    "    Validates that the number of rows matches the sum of TERM_SIZES.\n",
    "    Writes the updated dataset to the configured raw project-level dataset path.\n",
    "    \"\"\"\n",
    "    # Inputs / outputs\n",
    "    src_path = Path(\"output/interim_results/aggregated_metrics_v4_061025.csv\")\n",
    "    dst_path = Path(cfg.project_level_dataset_path_raw)\n",
    "\n",
    "    project_df = _read_dataset(src_path)\n",
    "\n",
    "    # Checks\n",
    "    term_sizes = cfg.TERM_SIZES\n",
    "    n_rows = len(project_df)\n",
    "\n",
    "    expected_total = sum(int(x) for x in term_sizes)\n",
    "    if expected_total != n_rows:\n",
    "        raise ValueError(\n",
    "            \"TERM_SIZES total does not match number of rows in the dataset.\\n\"\n",
    "            f\"sum(TERM_SIZES)={expected_total}, rows={n_rows}\\n\"\n",
    "            \"Fix TERM_SIZES or verify the input file has exactly one row per project.\"\n",
    "        )\n",
    "\n",
    "    if project_df['project index'].nunique() != n_rows:\n",
    "        print(\n",
    "            \"⚠️ Warning: Duplicated projects (Project Index is not unique to each entry)\"\n",
    "        )\n",
    "\n",
    "    # Add term column based on TERM_SIZES\n",
    "    # term_values: list[int] = []\n",
    "    # for term_idx, size in enumerate(term_sizes, start=1):\n",
    "    #     term_values.extend([term_idx] * int(size))\n",
    "\n",
    "    term_values = pd.Series(\n",
    "        [i for i, size in enumerate(term_sizes, start=1) for _ in range(size)]\n",
    "    )\n",
    "\n",
    "    project_df[\"term\"] = term_values\n",
    "\n",
    "    _write_dataset(project_df, dst_path)\n",
    "\n",
    "    # Report\n",
    "    counts = project_df[\"term\"].value_counts().sort_index()\n",
    "    print(f\"✅ Wrote project-level raw dataset with term column to: {dst_path}\")\n",
    "    print(f\"Rows: {n_rows} | Term distribution: {counts.to_dict()}\")\n",
    "\n",
    "# add_term_column_to_project_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d838558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From (669, 103) to (669, 104)\n"
     ]
    }
   ],
   "source": [
    "src = cfg.project_level_dataset_path_raw\n",
    "def add_comment_density(src: str) -> None:\n",
    "    df = _read_dataset(src)\n",
    "    row_in = df.shape\n",
    "    df['comment_density'] = df['3_folders_total_lines_of_comments'] / df['3_folders_total_lines']\n",
    "    print(f\"From {row_in} to {df.shape}\")\n",
    "    _write_dataset(df, src)\n",
    "\n",
    "# add_comment_density(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5f6b6",
   "metadata": {},
   "source": [
    "### Normalization Column Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8147027a",
   "metadata": {},
   "source": [
    "- replacing whitespace with underscores\n",
    "- converting uppercase letters to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0e598828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_columns(df: pd.DataFrame) -> list[str]:\n",
    "    return df.columns.tolist()\n",
    "\n",
    "\n",
    "# --- Project-level ---\n",
    "def normalize_project_df():\n",
    "    project_df_raw = _read_dataset(cfg.project_level_dataset_path_raw)\n",
    "    project_df_norm = normalize_dataframe_columns(project_df_raw)\n",
    "    _write_dataset(project_df_norm, cfg.project_level_dataset_path_normalized)\n",
    "    print(\n",
    "        f\"project_level: {project_df_raw.shape} -> {project_df_norm.shape} \"\n",
    "        f\"written to {cfg.project_level_dataset_path_normalized}\"\n",
    "    )\n",
    "    print(f\"columns before: {show_columns(project_df_raw)}\")\n",
    "    print(f\"columns after: {show_columns(project_df_norm)}\")\n",
    "\n",
    "\n",
    "# --- Commit-level ---\n",
    "def normalize_commit_df():\n",
    "    commit_df_raw = _read_dataset(cfg.commit_level_dataset_path_raw)\n",
    "    commit_df_norm = normalize_dataframe_columns(commit_df_raw)\n",
    "    _write_dataset(commit_df_norm, cfg.commit_level_dataset_path_normalized)\n",
    "    print(\n",
    "        f\"commit_level: {commit_df_raw.shape} -> {commit_df_norm.shape} \"\n",
    "        f\"written to {cfg.commit_level_dataset_path_normalized}\"\n",
    "    )\n",
    "    print(f\"columns before: {show_columns(commit_df_raw)}\")\n",
    "    print(f\"columns after: {show_columns(commit_df_norm)}\")\n",
    "\n",
    "\n",
    "# run only once whenever raw file changes\n",
    "# normalize_project_df()\n",
    "# normalize_commit_df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d917ce8a",
   "metadata": {},
   "source": [
    "### Commit level dataset fix (drop file_count, rename dmm column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a4f1ff",
   "metadata": {},
   "source": [
    "drop file_count as it is the same as files_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4e061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column not in dataframe anymore\n"
     ]
    }
   ],
   "source": [
    "def drop_doubled_col():\n",
    "    df = _read_dataset(cfg.commit_level_dataset_path_normalized)\n",
    "    # print(df.columns)\n",
    "    if 'file_count' in df.columns:\n",
    "        df = df.drop(columns=['file_count'])\n",
    "        print(\"dropped it\")\n",
    "    else:\n",
    "        print(\"column not in dataframe anymore\")\n",
    "    # print(df.columns)\n",
    "    _write_dataset(df, cfg.commit_level_dataset_path_normalized)\n",
    "\n",
    "# drop_doubled_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4ffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already has new names\n"
     ]
    }
   ],
   "source": [
    "path = cfg.commit_level_dataset_path_cleaned\n",
    "old_names = ['unit_size_(dmm)', 'complexity_(dmm)', 'interface_(dmm)']\n",
    "new_names = ['unit_size_dmm', 'complexity_dmm', 'interface_dmm']\n",
    "\n",
    "def rename_dmm_cols(path: str) -> None:\n",
    "    df = _read_dataset(path)\n",
    "    has_old = all(name in df.columns for name in old_names)\n",
    "    has_new = all(name in df.columns for name in new_names)\n",
    "    if has_old:\n",
    "        df = df.rename(columns={old_names[0]: new_names[0], old_names[1]: new_names[1], old_names[2]: new_names[2]})\n",
    "        _write_dataset(df=df, path=path)\n",
    "        print(\"successfully renamed\")\n",
    "    elif has_new:\n",
    "        print(\"already has new names\")\n",
    "    else:\n",
    "        print(f\"something is wrong - {df.columns}\")\n",
    "    \n",
    "\n",
    "# rename_dmm_cols(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b829a",
   "metadata": {},
   "source": [
    "### Add Dev Column to Commit Level DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5212b1",
   "metadata": {},
   "source": [
    "Meaning of Dev Column:\n",
    "| 'Dev' | Meaning |\n",
    "|----------|----------|\n",
    "| -1    | Bot  |\n",
    "| 0    | Not assignable  |\n",
    "| 1    | Dev 1 of project |\n",
    "| 2    | Dev 2 of project |\n",
    "| 3    | Dev 3 of project  |\n",
    "| ...    | ...  |\n",
    "\n",
    "optimal is if a project contains only bots and two developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "26bed6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column dev for testing add_dev_column_to_commit_df()\n",
    "def drop_dev_column_commit_df() -> None:\n",
    "    df_commits = _read_dataset(cfg.commit_level_dataset_path_normalized)\n",
    "    if \"dev\" in df_commits.columns:\n",
    "        df_commits = df_commits.drop(columns=[\"dev\"])\n",
    "        _write_dataset(df_commits, cfg.commit_level_dataset_path_normalized)\n",
    "        print(f\"Dropped 'dev' column from {cfg.commit_level_dataset_path_normalized}\")\n",
    "    else:\n",
    "        print(f\"No 'dev' column found in {cfg.commit_level_dataset_path_normalized}\")\n",
    "\n",
    "# drop_dev_column_commit_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c7820634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dev_column_to_commit_df()-> None:\n",
    "    # Import mapping df with dev column\n",
    "    filename = \"dev_mapping_manually.csv\"\n",
    "    dataset_path = cfg.base_path + \"output/interim_results/\" + filename\n",
    "    df_mapping = _read_dataset(dataset_path)\n",
    "    df_mapping = normalize_dataframe_columns(df_mapping)\n",
    "    \n",
    "    if \"dev\" not in df_mapping.columns: # check correctness of file\n",
    "        raise ValueError(\n",
    "            \"Mapping file must contain a 'dev' column after normalization \"\n",
    "            f\"(found: {list(df_mapping.columns)})\"\n",
    "        )\n",
    "\n",
    "    if len(df_mapping[df_mapping['dev'].isna()]) != 0: # check correctness of file\n",
    "        raise ValueError(\n",
    "            \"Mapping file must contain a 'dev' value for each entry, some dev values are missing \"\n",
    "            f\"(found: {list(df_mapping[df_mapping['dev'].isna()])})\"\n",
    "        )\n",
    "\n",
    "\n",
    "    # Import commit dataframe\n",
    "    df_commits = _read_dataset(cfg.commit_level_dataset_path_normalized)\n",
    "\n",
    "    if \"dev\" in df_commits.columns: # check correctness of file\n",
    "        raise ValueError(\n",
    "            \"Mapping file contains a 'dev' column indicates values were already added\"\n",
    "            f\"(found: {list(df_commits.columns)})\"\n",
    "        )\n",
    "\n",
    "    # For stats later\n",
    "    n_rows_in = len(df_commits)\n",
    "    n_col_in = df_commits.shape[1]\n",
    "    cols_in = set(df_commits.columns)\n",
    "\n",
    "    # Calculate bot list\n",
    "    # check which unique combination of (author name, author email) commited to more than one project\n",
    "    dev_projects = (\n",
    "        df_commits.dropna(subset=['project', 'author_name', 'author_email'])\n",
    "        .drop_duplicates(subset=['project', 'author_name', 'author_email'])  # one per project\n",
    "        .groupby(['author_name', 'author_email'])['project']\n",
    "        .nunique()\n",
    "        .reset_index(name='n_projects')\n",
    "    )\n",
    "\n",
    "    multi_project_devs = dev_projects[dev_projects['n_projects'] > 1].sort_values(\n",
    "        'n_projects', ascending=False\n",
    "    )\n",
    "\n",
    "    bots = multi_project_devs[multi_project_devs['n_projects'] > 3]\n",
    "    \n",
    "    # print(bots[['author_name', 'author_email']])\n",
    "\n",
    "    # bots don't need a project column, their dev column is equal to -1\n",
    "    bot_mapping = bots.copy()\n",
    "    bot_mapping = bot_mapping[['author_name', 'author_email']]\n",
    "    bot_mapping['dev'] = -1\n",
    "\n",
    "    # Extend commit dataframe with dev column from bot by matching on mail and name\n",
    "    df_commits = df_commits.merge(\n",
    "        bot_mapping,\n",
    "        on=['author_name', 'author_email'],\n",
    "        how='left',\n",
    "        validate=\"many_to_one\",\n",
    "    )\n",
    "    \n",
    "    # Extend commit dataframe with dev column by matching on project, mail and name\n",
    "    mapping_required = {\"project\", \"author_name\", \"author_email\", \"dev\"}\n",
    "    mapping_missing = mapping_required - set(df_mapping.columns) \n",
    "    if mapping_missing: # check that all required columns are present\n",
    "        raise ValueError(\n",
    "            \"Mapping dataset is missing required columns: \"\n",
    "            f\"{sorted(mapping_missing)} (found: {list(df_mapping.columns)})\"\n",
    "        )\n",
    "\n",
    "    df_mapping = df_mapping.rename(columns={\"dev\": \"dev_map\"})\n",
    "\n",
    "    df_commits = df_commits.merge( # creates dev_map column additional to dev column from bot\n",
    "        df_mapping,\n",
    "        on=['project', 'author_name', 'author_email'],\n",
    "        how=\"left\",\n",
    "        validate=\"many_to_one\",\n",
    "    )\n",
    "\n",
    "    df_mapping = df_mapping.rename(columns={\"dev_map\": \"dev\"})\n",
    "\n",
    "    df_commits[\"dev\"] = df_commits[\"dev\"].combine_first(df_commits[\"dev_map\"]) # merges dev and dev_map column, prioritize dev column from bot mapping\n",
    "    df_commits.drop(columns=[\"dev_map\"], inplace=True)\n",
    "\n",
    "    if len(df_commits[df_commits['dev'].isna()]) != 0: # check correctness of df\n",
    "        raise ValueError(\n",
    "            \"After merging dataframes dev column must contain a value for each entry, some dev values are missing \"\n",
    "            f\"(found: {list(df_commits[df_commits['dev'].isna()])})\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Stats\n",
    "    n_rows_out = len(df_commits)\n",
    "    n_col_out = df_commits.shape[1]\n",
    "    new_columns = set(df_commits.columns) - cols_in\n",
    "    n_dev_missing = int(df_commits[\"dev\"].isna().sum())\n",
    "    n_bots = len(bot_mapping)\n",
    "    dev_counts = (\n",
    "        df_commits[\"dev\"].fillna(\"NA\").value_counts(dropna=False).to_dict()    \n",
    "        if \"dev\" in df_commits.columns\n",
    "        else {}\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"✅ Added 'dev' column to commit-level dataset\\n\"\n",
    "        f\"Rows: {n_rows_in} -> {n_rows_out}\\n\"\n",
    "        f\"Cols: {n_col_in} -> {n_col_out}\\n\"\n",
    "        f\"New columns: {new_columns}\\n\"\n",
    "        f\"Missing dev: {n_dev_missing}\\n\"\n",
    "        f\"Bots flagged (dev=0): {n_bots}\\n\"\n",
    "        f\"Dev distribution: {dev_counts}\\n\"\n",
    "    )\n",
    "\n",
    "    # Store extended commit dataframe \n",
    "    _write_dataset(df_commits, cfg.commit_level_dataset_path_normalized)\n",
    "\n",
    "# run only once whenever raw file changes, check writting is enabled at bottom of function\n",
    "# add_dev_column_to_commit_df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f25b60f",
   "metadata": {},
   "source": [
    "Check which commit entries have 'dev' == -1, list all bots with how many commits they made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3c88dc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author_name       author_email                     \n",
       "autobot           autobot@students.cs.ubc.ca           887\n",
       "classy            classy@cs.ubc.ca                     486\n",
       "310-bot           autobot@students.cs.ubc.ca           457\n",
       "cs-310            cs-310-noreply@students.cs.ubc.ca    294\n",
       "CPSC 310 autobot  310-bot@students.cs.ubc.ca           219\n",
       "Service           autobot@students.cs.ubc.ca            51\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_commits = _read_dataset(cfg.commit_level_dataset_path_normalized)\n",
    "bot_list = df_commits[df_commits['dev'] == -1]\n",
    "bot_list[['author_name', 'author_email']].drop_duplicates()\n",
    "bot_list[['author_name', 'author_email']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5332741",
   "metadata": {},
   "source": [
    "### Clean Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890b4853",
   "metadata": {},
   "source": [
    "#### Commit Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "91701ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 20)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = _read_dataset(cfg.commit_level_dataset_path_normalized)\n",
    "df[df['project'] == 14].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9cbca6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of projects BEFORE removing incompletes:  681\n",
      "Number of commits BEFORE quantile filtering:  86483\n",
      "Number of projects AFTER removing incompletes:  681\n",
      "Number of commits AFTER quantile filtering:  57744\n"
     ]
    }
   ],
   "source": [
    "# remove merge commit\n",
    "print(\"Number of projects BEFORE removing incompletes: \", df[\"project\"].nunique())\n",
    "print(\"Number of commits BEFORE quantile filtering: \", df.shape[0])\n",
    "df = df[df['merge_commit'] == False]\n",
    "print(\"Number of projects AFTER removing incompletes: \", df[\"project\"].nunique())\n",
    "print(\"Number of commits AFTER quantile filtering: \", df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f53d6f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of projects BEFORE removing incompletes:  681\n",
      "Number of commits BEFORE quantile filtering:  57744\n",
      "Number of projects AFTER removing incompletes:  673\n",
      "Number of commits AFTER quantile filtering:  57736\n"
     ]
    }
   ],
   "source": [
    "# remove projects with only one commit\n",
    "print(\"Number of projects BEFORE removing incompletes: \", df[\"project\"].nunique())\n",
    "print(\"Number of commits BEFORE quantile filtering: \", df.shape[0])\n",
    "df = df[df.groupby(\"project\")[\"project\"].transform(\"size\") > 1]\n",
    "print(\"Number of projects AFTER removing incompletes: \", df[\"project\"].nunique())\n",
    "print(\"Number of commits AFTER quantile filtering: \", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "94ae69fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of commits BEFORE removing duplicates:  57736\n",
      "Number of projects BEFORE removing incompletes:  673\n",
      "Removed 4 projects with shared commits:\n",
      "[317 433 479 480]\n",
      "Number of commits AFTER removing duplicates:  57372\n",
      "Number of projects AFTER removing incompletes:  669\n"
     ]
    }
   ],
   "source": [
    "# remove projects with doubled hash\n",
    "# Remove all duplicates\n",
    "print(\"\\nNumber of commits BEFORE removing duplicates: \", df.shape[0])\n",
    "print(\"Number of projects BEFORE removing incompletes: \", df[\"project\"].nunique())\n",
    "# Identify and remove projects involved in shared commits\n",
    "shared_projects = (\n",
    "    df.loc[df['commit_hash'].duplicated(keep=False), 'project']\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "df = df[~df['project'].isin(shared_projects)]\n",
    "\n",
    "print(f\"Removed {len(shared_projects)} projects with shared commits:\\n{shared_projects}\")\n",
    "print(\"Number of commits AFTER removing duplicates: \", df.shape[0])\n",
    "print(\"Number of projects AFTER removing incompletes: \", df[\"project\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _write_dataset(df, cfg.commit_level_dataset_path_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb8ab4e",
   "metadata": {},
   "source": [
    "#### Project Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "71aa99a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = _read_dataset(cfg.project_level_dataset_path_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bf070f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['project_index', 'folder_count', 'file_count', 'file_count_tslike',\n",
       "       'file_count_.ts', 'file_count_.tsx', 'file_count_.js',\n",
       "       'file_count_.jsx', 'commit_count', 'unique_commit_authors',\n",
       "       ...\n",
       "       'typescript_total_lines_of_comments', 'typescript_blank_lines',\n",
       "       'javascript_file_count', 'javascript_total_lines', 'javascript_sloc',\n",
       "       'javascript_total_lines_of_comments', 'javascript_blank_lines',\n",
       "       'cc_sum', 'cc_sum_3folders', 'term'],\n",
       "      dtype='object', length=103)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a644e12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of projects BEFORE removing incompletes:  681\n",
      "Number of projects AFTER removing incompletes:  677\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of projects BEFORE removing incompletes: \", df[\"project_index\"].nunique())\n",
    "df = df[~df['project_index'].isin(shared_projects)]\n",
    "print(\"Number of projects AFTER removing incompletes: \", df[\"project_index\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2498e72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of projects BEFORE removing incompletes:  677\n",
      "Number of projects AFTER removing incompletes:  669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(669, 103)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove projects with only one commit\n",
    "print(\"Number of projects BEFORE removing incompletes: \", df[\"project_index\"].nunique())\n",
    "\n",
    "df = df[df[\"commit_count\"] > 1]\n",
    "\n",
    "print(\"Number of projects AFTER removing incompletes: \", df[\"project_index\"].nunique())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c5fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _write_dataset(df, cfg.project_level_dataset_path_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9a4c5596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['genai_period', 'term', 'project', 'author_name', 'author_email',\n",
       "       'date', 'repository', 'insertions', 'deletions', 'total_lines',\n",
       "       'files_changed', 'diff_lines', 'file_count', 'unit_size_(dmm)',\n",
       "       'complexity_(dmm)', 'interface_(dmm)', 'commit_hash', 'merge_commit',\n",
       "       'default_branch', 'dev'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8818b18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "files_changed\n",
       "1       25803\n",
       "2       14443\n",
       "3       10419\n",
       "4        6479\n",
       "5        4675\n",
       "        ...  \n",
       "1029        1\n",
       "203         1\n",
       "256         1\n",
       "6026        1\n",
       "5993        1\n",
       "Name: count, Length: 330, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['files_changed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1630f101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_count\n",
       "1       25803\n",
       "2       14443\n",
       "3       10419\n",
       "4        6479\n",
       "5        4675\n",
       "        ...  \n",
       "1029        1\n",
       "203         1\n",
       "256         1\n",
       "6026        1\n",
       "5993        1\n",
       "Name: count, Length: 330, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['file_count'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
